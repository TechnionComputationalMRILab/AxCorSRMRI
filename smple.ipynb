{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'resample_cases' from 'PreprocessUtils' (/argusdata/users/jenny075/JennySh/code_production/PreprocessUtils.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdistutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPreprocessUtils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m resample_cases,create_database\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmain\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'resample_cases' from 'PreprocessUtils' (/argusdata/users/jenny075/JennySh/code_production/PreprocessUtils.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "import distutils.version\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PreprocessUtils import resample_cases,create_database\n",
    "from main import *\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-08T22:32:45.255923500Z",
     "start_time": "2024-06-08T22:32:45.212613500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training and data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Date Preprocess\n",
    "\n",
    "First time users, here's a guide to organizing your data effectively:\n",
    "*Create a New Folder* : Open a new folder where you'll store your data.\n",
    "*Case ID Consistency* : Make sure the Case ID is the same for both axial and coronal files, and it's at the beginning of the file name. This simplifies data management and retrieval.\n",
    "*File Naming Convention* : Use a consistent naming convention for your files. For axial files, include \"AXIAL\", \"Axial\", \"axial\", or similar after the case_id of the file name. For coronal files, use \"CORONAL\", \"Coronal\", \"coronal\", or similar. This makes it easy to distinguish between the two types of files."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resample isotropicly nifti coronal files\n",
    "path_to_data_files = \"/tcmldrive/shared/RambamMRE082022/new2/\"\n",
    "coronal_files_prefix = None # not mandatory\n",
    "ResampleCases(path_dir = path_to_data_files ,prefix = coronal_files_prefix)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-08T22:29:19.870484500Z",
     "start_time": "2024-06-08T22:26:56.184238500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, you split the data into training, validation, and test datasets by assigning values to train_frac and test_frac. If you want to train on all the available data and then apply the trained model to the entire dataset, you would set train_frac to 1 and test_frac to"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Creat DB file with isotropic, coronal and axial files paths\n",
    "path_to_data_files = \"/tcmldrive/shared/RambamMRE082022/new2/\"\n",
    "CreateDateBase(path_to_data_files,cor_prefix=None,ax_prefix=None,train_frac=1,test_frac=0,num_folds = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-08T22:29:23.153633100Z",
     "start_time": "2024-06-08T22:29:23.046814800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paramaters Initializaion\n",
    "\n",
    "In this section, you have the option to adjust the main parameters for both the framework and model training. You can explore additional parameters in the parameter_dictionary.txt file. The default parameters for training are already predefined."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "override_args = {\n",
    "    \"path_to_set\":\"/tcmldrive/shared/RambamMRE082022/new2/\",\n",
    "    \"path_to_results\":\"/argusdata/users/jenny075/JennySh/results/\",\n",
    "    \"amount_of_files\":20,\n",
    "    \"batch_size\":12,\n",
    "    \"gpu_device\":\"0,1\",\n",
    "    \"title\":\"Test\",\n",
    "    \"total_samples\":1000,\n",
    "    \"patch_size\":48,\n",
    "    \"epochs\": 5,\n",
    "    \"valid_batch_size\": 40,\n",
    "    \"val_epoch\":5,\n",
    "}\n",
    "\n",
    "parser = setup_parser()\n",
    "args, _ = parser.parse_known_args([])\n",
    "vars(args).update(override_args)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-08T22:30:23.613521Z",
     "start_time": "2024-06-08T22:30:23.573353Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Creation\n",
    "\n",
    "This function creates training, validation, and test datasets for model training. It also generates a new folder at the specified 'path_to_results' to save the results. Additionally, it sets up a writer for TensorBoard tracking.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "config.multi_gpu -  [0, 1]\n",
      "gpu available: True\n",
      "gpu count: 2\n",
      "config.resume -  False\n",
      "Building and creating ESRT model from scratch\n",
      "Load train dataset and valid dataset...\n",
      "path_to_db- /tcmldrive/shared/RambamMRE082022/new2/DB.csv\n",
      "length train_list -  1000\n",
      "length val_list -  2\n",
      "length test_list -  2\n",
      "length train_list volume -  2\n",
      "length val_list volume -  2\n",
      "length test_list volume -  2\n",
      "Load train dataset and valid dataset...\n",
      "Load train dataset and valid dataset successfully.\n",
      "Finish data peparation and parameters Initializaion\n"
     ]
    }
   ],
   "source": [
    "dl_train , dl_valid_lr,dl_valid_hr,dl_test_lr,dl_test_hr,result_dir,writer,config  = Data_Inittializaion(args)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-08T22:35:47.411863Z",
     "start_time": "2024-06-08T22:33:20.982321500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train validation and test\n",
    "This function first, initialize the model and set the model parameters. Then, trains the model using the dl_train dataset and validate it using dl_valid_lr and dl_valid_hr datasets. Once training is complete, it tests the model using dl_test_lr and dl_test_hr datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [43]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtraining_validation_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdl_train\u001B[49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdl_valid_lr\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdl_valid_hr\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdl_test_lr\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdl_test_hr\u001B[49m\u001B[43m,\u001B[49m\u001B[43mresult_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/argusdata/users/jenny075/JennySh/code_production/main.py:467\u001B[0m, in \u001B[0;36mtraining_validation_test\u001B[0;34m(dl_train, dl_valid_lr, dl_valid_hr, dl_test_lr, dl_test_hr, result_dir, writer, args)\u001B[0m\n\u001B[1;32m    465\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m config\u001B[38;5;241m.\u001B[39mval_epoch \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m :\n\u001B[1;32m    466\u001B[0m     start_val \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 467\u001B[0m     fid, kid, step, max_size_lr, max_size_hr \u001B[38;5;241m=\u001B[39m \u001B[43mvalidate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mInceptionV3_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdl_valid_lr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    468\u001B[0m \u001B[43m                                                                 \u001B[49m\u001B[43mdl_valid_hr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    469\u001B[0m \u001B[43m                                                                 \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    470\u001B[0m \u001B[43m                                                                 \u001B[49m\u001B[43mresult_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    471\u001B[0m \u001B[43m                                                                 \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    472\u001B[0m \u001B[43m                                                                 \u001B[49m\u001B[43mmax_size_lr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_size_hr\u001B[49m\u001B[43m,\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    473\u001B[0m     end_val \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m    474\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch val time-\u001B[39m\u001B[38;5;124m'\u001B[39m, time\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mS\u001B[39m\u001B[38;5;124m\"\u001B[39m, time\u001B[38;5;241m.\u001B[39mgmtime(end_val \u001B[38;5;241m-\u001B[39m start_val)))\n",
      "File \u001B[0;32m/argusdata/users/jenny075/JennySh/code_production/TrainingFunctions.py:710\u001B[0m, in \u001B[0;36mvalidate\u001B[0;34m(model, InceptionV3_model, valid_dataloader_lr, valid_dataloader_hr, epoch, writer, step, result_dir, patch_size, max_size_lr, max_size_hr, config)\u001B[0m\n\u001B[1;32m    707\u001B[0m rec_mid_tot\u001B[38;5;241m.\u001B[39mappend(rec_mid)\n\u001B[1;32m    708\u001B[0m rec_mid_tot_max_size\u001B[38;5;241m.\u001B[39mappend(pad_to_max_size(lr_1_rec, max_size_hr))\n\u001B[1;32m    709\u001B[0m rec_sr \u001B[38;5;241m=\u001B[39m recon_im_torch_rectangle_gaus(sr\u001B[38;5;241m.\u001B[39msqueeze(), \u001B[38;5;28mint\u001B[39m(size_lr[\u001B[38;5;241m0\u001B[39m]), \u001B[38;5;28mint\u001B[39m(size_lr[\u001B[38;5;241m1\u001B[39m]),\n\u001B[0;32m--> 710\u001B[0m                                              calculate_overlap(\u001B[38;5;28mint\u001B[39m(size_lr[\u001B[38;5;241m0\u001B[39m]), patch_size),\n\u001B[1;32m    711\u001B[0m                                              calculate_overlap(\u001B[38;5;28mint\u001B[39m(size_lr[\u001B[38;5;241m1\u001B[39m]), patch_size),\n\u001B[1;32m    712\u001B[0m                                              device \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mdevice)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(config\u001B[38;5;241m.\u001B[39mdevice,\n\u001B[1;32m    713\u001B[0m                                                                                               non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\n\u001B[1;32m    715\u001B[0m rec_sr_resize \u001B[38;5;241m=\u001B[39m pad_to_max_size(rec_sr, max_size_lr)\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\n\u001B[1;32m    716\u001B[0m rec_sr_tot\u001B[38;5;241m.\u001B[39mappend(rec_sr_resize)\n",
      "File \u001B[0;32m/argusdata/users/jenny075/JennySh/code_production/TrainingFunctions.py:375\u001B[0m, in \u001B[0;36mrecon_im_torch_rectangle_gaus\u001B[0;34m(patches, im_h, im_w, stride_x, stride_y, device)\u001B[0m\n\u001B[1;32m    373\u001B[0m patch_weight \u001B[38;5;241m=\u001B[39m gkern(patches\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m],patches\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m2\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    374\u001B[0m patch_weight \u001B[38;5;241m=\u001B[39m (torch\u001B[38;5;241m.\u001B[39mones(patches\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;241m*\u001B[39mpatch_weight)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m--> 375\u001B[0m patches \u001B[38;5;241m=\u001B[39m patch_weight\u001B[38;5;241m*\u001B[39mpatches\n\u001B[1;32m    376\u001B[0m patches \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mpermute(patches,(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m0\u001B[39m))\n\u001B[1;32m    377\u001B[0m patches \u001B[38;5;241m=\u001B[39mpatches\u001B[38;5;241m.\u001B[39mreshape(patches\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m*\u001B[39mpatches\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m1\u001B[39m],patches\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m/tcmldrive/JennySh/conda_envs/MRI_SR/lib/python3.8/site-packages/torch/_tensor.py:649\u001B[0m, in \u001B[0;36mTensor.__array_wrap__\u001B[0;34m(self, array)\u001B[0m\n\u001B[1;32m    645\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    647\u001B[0m \u001B[38;5;66;03m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001B[39;00m\n\u001B[1;32m    648\u001B[0m \u001B[38;5;66;03m# `numpy.sin(tensor) -> tensor` or `numpy.greater(tensor, 0) -> ByteTensor`\u001B[39;00m\n\u001B[0;32m--> 649\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__array_wrap__\u001B[39m(\u001B[38;5;28mself\u001B[39m, array):\n\u001B[1;32m    650\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    651\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(Tensor\u001B[38;5;241m.\u001B[39m__array_wrap__, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m, array\u001B[38;5;241m=\u001B[39marray)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "training_validation_test(dl_train , dl_valid_lr,dl_valid_hr,dl_test_lr,dl_test_hr,result_dir,writer,config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reconstruct SR Volumes\n",
    "In this section, you can apply the trained model to all the data in the folder. Ensure that the data is isotropically resampled before applying the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Framework Initializaion\n",
    "\n",
    "In this section, you need to det the main parameters for both the framework . The default parameters for the framework are already predefined."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "override_args_test = {\n",
    "    \"path_to_set\":\"/tcmldrive/shared/RambamMRE082022/new3/\",\n",
    "    \"path_to_results\":\"/argusdata/users/jenny075/JennySh/results/\",\n",
    "    \"path_to_trained_model\":\"/argusdata/users/jenny075/JennySh/DGX_results/RAMBAM_COR_FINAL_CHECKPOINTS/ESRT_RAMBAM_FINAL_CORONAL_CHEACKPOINTS_fold1__25_04_2024_18_41/Saved/check_points/1000.pth\",\n",
    "    \"gpu_device\":\"0,1\",\n",
    "    \"title\":\"Test_SR_REc\",\n",
    "    \"save_nifti\":True,\n",
    "}\n",
    "\n",
    "parser = setup_parser_test()\n",
    "args_, _ = parser.parse_known_args([])\n",
    "vars(args_).update(override_args_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reconstruction\n",
    "\n",
    "This function applies the model to the isotropically resampled files and saves the super-resolution version of each file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reconstract_SR_volumes_in_folder(args_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
