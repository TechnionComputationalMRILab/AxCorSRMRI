{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AxCoRSMRI sample notebook\n",
    "\n",
    "Self-Supervised Realistic Through-Plane MRI Super Resolution from Clinical 2D Axial and Coronal Acquisition.\n",
    "\n",
    "[For more information, please see the GitHub repository.](https://github.com/TechnionComputationalMRILab/AxCorSRMRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T05:49:42.171144200Z",
     "start_time": "2024-06-02T05:49:14.970468300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "import torch\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Preprocess\n",
    "\n",
    "For first time users, please prepare your data accordingly. \n",
    "\n",
    "Create a where each case ID has two files - one for axial and one for coronal. The files should be in nifti format (.nii.gz). The files should be named as follows: \"case_id_AXIAL.nii.gz\" and \"case_id_CORONAL.nii.gz\".\n",
    "\n",
    "For example:\n",
    "```\n",
    "    - data_folder\n",
    "        - 1\n",
    "            - 1_AXIAL.nii.gz\n",
    "            - 1_CORONAL.nii.gz\n",
    "        - 2\n",
    "            - 2_AXIAL.nii.gz\n",
    "            - 2_CORONAL.nii.gz\n",
    "        - 3\n",
    "            - 3_AXIAL.nii.gz\n",
    "            - 3_CORONAL.nii.gz\n",
    "        - 4\n",
    "            - 4_AXIAL.nii.gz\n",
    "            - 4_CORONAL.nii.gz\n",
    "        - 5\n",
    "            - 5_AXIAL.nii.gz\n",
    "            - 5_CORONAL.nii.gz\n",
    "```\n",
    "\n",
    "Note that you need to have write permissions in the `data_folder` directory, as the code will write the preprocessed data in the same directory. The paths require a trailing slash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T16:58:16.242766Z",
     "start_time": "2024-06-01T16:57:54.210407700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Resample isotropic NIFTI coronal files\n",
    "# from axcorsrmri import resample_cases\n",
    "\n",
    "# path_to_data_files = r\"./data/\"\n",
    "# resample_cases(path_dir = path_to_data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T05:49:42.343514500Z",
     "start_time": "2024-06-02T05:49:42.185775600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Create DB file with isotropic, coronal and axial files paths\n",
    "# from axcorsrmri import create_database\n",
    "\n",
    "# path_to_data_files = r\"./data/\"\n",
    "# create_database(\n",
    "#     path_to_data_files,\n",
    "#     train_frac=0.8,\n",
    "#     test_frac=0.1,\n",
    "#     num_folds=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Param Initializaion\n",
    "\n",
    "Here you can define the main parameters for the model training. Please refer to `Parameters_dictionary.txt` for more details of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T05:49:42.360005100Z",
     "start_time": "2024-06-02T05:49:42.360005100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from axcorsrmri import parser_setup\n",
    "\n",
    "override_args = {\n",
    "    \"path_to_set\": r\"./data/\",\n",
    "    \"path_to_results\": r\"./results/\",\n",
    "    \"amount_of_files\":20,\n",
    "    \"batch_size\":12,\n",
    "    \"loss\":\"L2\",\n",
    "    \"gpu_device\":\"0\",\n",
    "    \"amount_of_slices\":3,\n",
    "    \"title\":\"Test\",\n",
    "    \"total_samples\":100,\n",
    "    \"patch_size\":48,\n",
    "    \"epochs\": 20,\n",
    "    \"lr_g\":0.0001,\n",
    "    \"lr_d\": 0.0001,\n",
    "    \"scheduler\": \"const\",\n",
    "    \"max_workers_train\": 12,\n",
    "    \"max_workers_valid\": 30,\n",
    "    \"valid_batch_size\": 40,\n",
    "    \"adversarial_weight_I\": 0.02,\n",
    "    \"adversarial_weight_E\": 0.02,\n",
    "    \"d_optimizer_step_size\":40 ,\n",
    "    \"g_optimizer_step_size\": 160,\n",
    "    \"val_epoch\": 5,\n",
    "    \"image_save_freq_batch\": 100,\n",
    "    \"mage_save_freq_epoch\": 5,\n",
    "    \"save_tensor\":True,\n",
    "    \"save_nifti\":True\n",
    "}\n",
    "\n",
    "args = parser_setup(override_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset Creation\n",
    "\n",
    "Initialize the training, validation and test datasets for the model training. In addition, the function creates a new folder in `path_to_results` to save the results and logs for tracking with tensorboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T05:51:43.042472900Z",
     "start_time": "2024-06-02T05:49:42.360005100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and creating ESRT model from scratch.\n",
      "Loading the training and validation datasets...\n",
      "Loading the training and validation datasets...\n",
      "Loaded training and validation datasets successfully.\n",
      "Finished data preparation and parameters initialization.\n",
      "CPU times: user 2min 52s, sys: 1min 32s, total: 4min 24s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from axcorsrmri import initialize_data\n",
    "dl_train, dl_valid_lr, dl_valid_hr, dl_test_lr, dl_test_hr, result_dir, writer, config = initialize_data(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training, validation, and testing\n",
    "Initalize the model and model parameters, and then train it on `dl_train` and validate it on `dl_valid_lr` and `dl_valid_hr`. After the training is done, it is tested on `dl_test_lr` and `dl_test_hr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Built model successfully.\n",
      "Defining all optimizer functions...\n",
      "Defined all optimizer functions successfully.\n",
      "Defining all optimizer scheduler functions...\n",
      "Defined all optimizer scheduler functions successfully.\n",
      "Defining all loss functions...\n",
      "Defined all loss functions successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::_amp_foreach_non_finite_check_and_unscale_' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_amp_foreach_non_finite_check_and_unscale_' is only available for these backends: [CUDA, BackendSelect, Named, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCUDA: registered at /pytorch/build/aten/src/ATen/RegisterCUDA.cpp:20674 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:60 [backend fallback]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradMLC: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradHPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradNestedTensor: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:9750 [kernel]\nAutocast: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:255 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/BatchingRegistrations.cpp:1019 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m/tcmldrive/users/ang.a/AxCorSRMRI/main.py:450\u001b[0m, in \u001b[0;36mtraining_validation_test\u001b[0;34m(dl_train, dl_valid_lr, dl_valid_hr, dl_test_lr, dl_test_hr, result_dir, writer, args)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m trange(start_epoch, config\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m    448\u001b[0m     start_train \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 450\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscriminator_I\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdiscriminator_E\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcolor_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m          \u001b[49m\u001b[43madversarial_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m          \u001b[49m\u001b[43md_optimizer_I\u001b[49m\u001b[43m,\u001b[49m\u001b[43md_optimizer_E\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m          \u001b[49m\u001b[43mg_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43md_scheduler_I\u001b[49m\u001b[43m,\u001b[49m\u001b[43md_scheduler_E\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m          \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m          \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m          \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m          \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     end_train \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# print('Epoch train time-', time.strftime(\"%H:%M:%S\", time.gmtime(end_train - start_train)))\u001b[39;00m\n",
      "File \u001b[0;32m/tcmldrive/users/ang.a/AxCorSRMRI/TrainingFunctions.py:502\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(discriminator_I, discriminator_E, generator, train_dataloader, color_criterion, adversarial_criterion, d_optimizer_I, d_optimizer_E, g_optimizer, d_scheduler_I, d_scheduler_E, g_scheduler, scheduler_type, loss_type, epoch, scaler, writer, config)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# Gradient zoom\u001b[39;00m\n\u001b[1;32m    501\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m(d_loss_hr_I\u001b[38;5;241m+\u001b[39md_loss_sr_I))\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 502\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_optimizer_I\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m scale \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mget_scale()\n\u001b[1;32m    504\u001b[0m d_scheduler_I\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.conda/envs/axcorsrmri/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:334\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState\u001b[38;5;241m.\u001b[39mREADY:\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/axcorsrmri/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:279\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    276\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    277\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((\u001b[38;5;241m1\u001b[39m,), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 279\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[0;32m~/.conda/envs/axcorsrmri/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:224\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m device, per_dtype_grads \u001b[38;5;129;01min\u001b[39;00m per_device_and_dtype_grads\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m grads \u001b[38;5;129;01min\u001b[39;00m per_dtype_grads\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 224\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_amp_foreach_non_finite_check_and_unscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mper_device_found_inf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mper_device_inv_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m per_device_found_inf\u001b[38;5;241m.\u001b[39m_per_device_tensors\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::_amp_foreach_non_finite_check_and_unscale_' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_amp_foreach_non_finite_check_and_unscale_' is only available for these backends: [CUDA, BackendSelect, Named, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCUDA: registered at /pytorch/build/aten/src/ATen/RegisterCUDA.cpp:20674 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:60 [backend fallback]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradMLC: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradHPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradNestedTensor: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:9750 [kernel]\nAutocast: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:255 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/BatchingRegistrations.cpp:1019 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from axcorsrmri import training_validation_test\n",
    "training_validation_test(dl_train, dl_valid_lr, dl_valid_hr, dl_test_lr, dl_test_hr, result_dir, writer, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reconstruct SR Volumes\n",
    "Apply the SR model on all `isotropic` files in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from axcorsrmri import test_parser_setup\n",
    "\n",
    "override_args_test = {\n",
    "    \"path_to_set\": r\"./data/\",\n",
    "    \"path_to_results\": r\"./results/\",\n",
    "    \"path_to_trained_model\": r\"./Test_06_06_2024_11_47/\",\n",
    "    \"amount_of_files\":20,\n",
    "    \"batch_size\":12,\n",
    "    \"gpu_device\":\"0,1\",\n",
    "    \"amount_of_slices\":3,\n",
    "    \"title\":\"Test\",\n",
    "    \"save_tensor\":True,\n",
    "    \"save_nifti\":True,\n",
    "}\n",
    "\n",
    "test_args = test_parser_setup(override_args_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maxcorsrmri\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reconstruct_SR_volumes_in_folder\n\u001b[0;32m----> 3\u001b[0m \u001b[43mreconstruct_SR_volumes_in_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tcmldrive/users/ang.a/AxCorSRMRI/main.py:692\u001b[0m, in \u001b[0;36mreconstruct_SR_volumes_in_folder\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;66;03m# print(\"config.multi_gpu - \", config.multi_gpu)\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;66;03m# print(\"config.use_multi_gpu {}\".format(config.use_multi_gpu))\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m# if torch.cuda.is_available():\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m#     print('gpu count:', str(torch.cuda.device_count()))\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating the dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 692\u001b[0m list_test_slices,list_test_volume,test_file_to_idx \u001b[38;5;241m=\u001b[39m \u001b[43mmake_list_to_reconstract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m istropic_dataset \u001b[38;5;241m=\u001b[39m DatasetCreation\u001b[38;5;241m.\u001b[39mCustomDataset_Test(slices\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_of_consecutive_slices, file_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m(list_test_slices),\n\u001b[1;32m    694\u001b[0m                                                    lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, file_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnifti\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    695\u001b[0m                                                    batch\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size, patch_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpatch_size,\n\u001b[1;32m    696\u001b[0m                                                    use_db\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    697\u001b[0m                                                    list_volumes\u001b[38;5;241m=\u001b[39mlist_test_volume, file_to_idx\u001b[38;5;241m=\u001b[39mtest_file_to_idx)\n\u001b[1;32m    698\u001b[0m isotropic_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset\u001b[38;5;241m=\u001b[39mistropic_dataset, batch_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m    699\u001b[0m                                        num_workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_workers_test,prefetch_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m/tcmldrive/users/ang.a/AxCorSRMRI/DatasetCreation.py:560\u001b[0m, in \u001b[0;36mmake_list_to_reconstract\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    557\u001b[0m             path_to_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mpath_to_set,file)\n\u001b[1;32m    558\u001b[0m             list_of_files\u001b[38;5;241m.\u001b[39mappend(path_to_file)\n\u001b[0;32m--> 560\u001b[0m list_test_slices \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_patch_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_of_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_of_consecutive_slices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mRec_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m list_test_volume, test_file_to_idx \u001b[38;5;241m=\u001b[39m create_patch_list_and_volume_list(list_of_files, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    562\u001b[0m                                                                        patch_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpatch_size,Rec_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m list_test_slices,list_test_volume,test_file_to_idx\n",
      "File \u001b[0;32m/tcmldrive/users/ang.a/AxCorSRMRI/DatasetCreation.py:227\u001b[0m, in \u001b[0;36mcreate_patch_list\u001b[0;34m(list_files, max_patches, num_of_consecutive_slices, train, patch_size, state, Rec_dir)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Rec_dir :\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(list_files)):\n\u001b[0;32m--> 227\u001b[0m         temp_list_1 \u001b[38;5;241m=\u001b[39m \u001b[43mreturn_list_of_valuble_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_of_consecutive_slices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m         total_1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(temp_list_1)\n\u001b[1;32m    229\u001b[0m         coustom_slices_1\u001b[38;5;241m.\u001b[39mextend(temp_list_1)\n",
      "File \u001b[0;32m/tcmldrive/users/ang.a/AxCorSRMRI/DatasetCreation.py:105\u001b[0m, in \u001b[0;36mreturn_list_of_valuble_slices\u001b[0;34m(file_path, num_of_consecutive_slices, axis)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mThis function return list of all the non-zero slices in the volume in the following form\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m[file_path,slice_number in the volume, augmentation state [1], [slice_width, slice_highet]]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m temp_file \u001b[38;5;241m=\u001b[39m load_nifti_image(file_path,  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnifti\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m first_slice, last_slice \u001b[38;5;241m=\u001b[39m \u001b[43mnon_zero_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# if temp_file.shape[1] % 2 !=0:\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m#     temp_file = temp_file[:,:-1,:]\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# if temp_file.shape[2] % 2 !=0:\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m#     temp_file = temp_file[:,:,:-1]\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_slice \u001b[38;5;241m>\u001b[39m num_of_consecutive_slices:\n",
      "File \u001b[0;32m/tcmldrive/users/ang.a/AxCorSRMRI/DatasetCreation.py:52\u001b[0m, in \u001b[0;36mnon_zero_slice\u001b[0;34m(vol, axis)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnon_zero_slice\u001b[39m(vol,axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    This function receives a volume and return the first and last slices with non zero values on spacific direction.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    :param vol: 3d array\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    :param axis: direction to check (0-axial,1-coronal,2 segittal)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    :return: first_slice first non zero slice,last_slice last non zero slice\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     non_zero \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     first_slice \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(non_zero[axis])\n\u001b[1;32m     54\u001b[0m     last_slice \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(non_zero[axis])\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mnonzero\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/axcorsrmri/lib/python3.8/site-packages/numpy/core/fromnumeric.py:1919\u001b[0m, in \u001b[0;36mnonzero\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   1827\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_nonzero_dispatcher)\n\u001b[1;32m   1828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnonzero\u001b[39m(a):\n\u001b[1;32m   1829\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;124;03m    Return the indices of the elements that are non-zero.\u001b[39;00m\n\u001b[1;32m   1831\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1917\u001b[0m \n\u001b[1;32m   1918\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnonzero\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/axcorsrmri/lib/python3.8/site-packages/numpy/core/fromnumeric.py:58\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from axcorsrmri import reconstruct_SR_volumes_in_folder\n",
    "\n",
    "reconstruct_SR_volumes_in_folder(test_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
